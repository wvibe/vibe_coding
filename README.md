# ğŸ§ª VibeLab Coding Playground (`vibelab`)

Welcome to **VibeLab's** playground!
This repository is a sandbox for experimenting with ideas, learning new tech, and storing useful code snippets.

---

## ğŸ¯ Purpose

* Prototype new concepts quickly
* Explore different tool-chains and libraries
* Practise coding techniques
* Keep reference examples for future projects

---

## ğŸš€ Getting Started

### 1 Create a virtual environment (conda recommended)

```bash
conda create -n vbl python=3.12
conda activate vbl
```

### 2 Clone and install

```bash
git clone --recursive https://github.com/wvibe/vibe_coding.git
cd vibe_coding

# Installs every dependency *and* sets up editable packages
pip install -r requirements.txt
```

`requirements.txt` contains

```
-e .                   # editable install of src/vibelab
-e ./ext/ultralytics   # editable install of our forked Ultralytics submodule
-e ./ext/qwen2.5-vl/qwen-vl-utils  # editable install of Qwen VL utilities
<all third-party packages>
```

---

## ğŸ—‚ Project Structure

```
vibe_coding/
â”œâ”€ configs/            # dataset / training / model YAML
â”œâ”€ docs/               # design docs & guides
â”œâ”€ ext/
â”‚   â”œâ”€ ultralytics/    # YOLO models (fork)
â”‚   â””â”€ qwen2.5-vl/     # Qwen VLM repository (submodule)
â”œâ”€ notebooks/          # Jupyter experiments
â”‚   â””â”€ vlm/            # Vision Language Model experiments
â”œâ”€ ref/                # read-only reference code
â”œâ”€ scripts/            # CLI utilities (not part of the package)
â”œâ”€ src/
â”‚   â”œâ”€ vibelab/        # â† top-level Python package (new)
â”‚   â”‚   â”œâ”€ models/
â”‚   â”‚   â””â”€ utils/
â”‚   â””â”€ â€¦legacy code    # to be moved into `vibelab`
â”œâ”€ tests/              # mirrors src/ hierarchy
â”œâ”€ requirements.txt
â””â”€ setup.py
```

*New code should live under **`src/vibelab/â€¦`** and be imported as, for example:*

```python
from vibelab.utils.geometry import mask_to_yolo_polygons
```

Legacy `from src.â€¦` imports will be migrated gradually.

---

## âš™ï¸ Environment Variables

Create a `.env` file in the repo root (example):

```dotenv
VIBE_ROOT=/abs/path/to/vibe
VHUB_ROOT=${VIBE_ROOT}/vhub
DATA_ROOT=${VHUB_ROOT}/data
```

---

## ğŸ¤– Vision Language Models (VLM)

This repository includes support for Vision Language Models, particularly Qwen2.5-VL:

### Features
- **Multi-modal understanding**: Process images, videos, and text together
- **Fine-tuning capabilities**: Train VLMs on custom datasets using TRL/PEFT
- **Video processing**: Support for long videos with temporal understanding
- **Document parsing**: Advanced OCR and document understanding

### Quick Start
```python
from qwen_vl_utils import process_vision_info
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor

# Load model and processor
model = Qwen2_5_VLForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

# Process multimodal input
messages = [{"role": "user", "content": [
    {"type": "image", "image": "path/to/image.jpg"},
    {"type": "text", "text": "Describe this image."}
]}]
```

### Notebooks
Check out `notebooks/vlm/` for example notebooks including:
- Fine-tuning VLMs with TRL
- Multi-modal data processing
- Video understanding examples

---

## ğŸ–¥ï¸ Cursor IDE Setup

For an enhanced experience with the Cursor IDE, follow the guide:

â¡ï¸ **docs/cursor/setup-guide.md**

---

## ğŸ‘¥ Contributing

* Work on a feature branch
* Keep commits focused and documented
* Feel free to add new folders for distinct experiments
* Update docs / notebooks with your findings

---

## YOLO Training Scripts

The training scripts now live under **`vibelab/models/ext/â€¦`**.
Update your command lines accordingly, e.g.:

```bash
python src/vibelab/models/ext/yolov8/train_yolov8.py \
  --config src/vibelab/models/ext/yolov8/configs/voc_finetune_config.yaml \
  --name yolov8l_voc_finetune_run1
```

(Full instructions remain the same as before.)

---

## ğŸ“ Note

This is an experimental playgroundâ€”break things, fix them, and learn!

Happy coding! âœ¨