{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASCAL VOC 2007 数据集探索\n",
    "\n",
    "本笔记本探索PASCAL VOC 2007数据集，特别关注目标检测和两种分割任务(语义分割和实例分割)的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Using regular tqdm instead of tqdm.notebook\n",
    "\n",
    "# Set display parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC dataset path: /Users/weimu/vibe/data/VOCdevkit/VOC2007\n"
     ]
    }
   ],
   "source": [
    "# Set VOC2007 dataset path\n",
    "# Modify this according to your actual path\n",
    "VOC_ROOT = '/Users/weimu/vibe/data/VOCdevkit/VOC2007'\n",
    "\n",
    "# Check if path exists\n",
    "if not os.path.exists(VOC_ROOT):\n",
    "    print(f\"Warning: Path {VOC_ROOT} does not exist, please verify the correct VOC dataset path\")\n",
    "else:\n",
    "    print(f\"VOC dataset path: {VOC_ROOT}\")\n",
    "\n",
    "# Define dataset subdirectories\n",
    "IMAGE_DIR = os.path.join(VOC_ROOT, 'JPEGImages')\n",
    "ANNOTATION_DIR = os.path.join(VOC_ROOT, 'Annotations')\n",
    "SEGMENTATION_DIR = os.path.join(VOC_ROOT, 'SegmentationClass')\n",
    "SEGMENTATION_INSTANCE_DIR = os.path.join(VOC_ROOT, 'SegmentationObject')\n",
    "IMAGESETS_DIR = os.path.join(VOC_ROOT, 'ImageSets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image directory (/Users/weimu/vibe/data/VOCdevkit/VOC2007/JPEGImages): Exists - Contains 9963 files\n",
      "Annotation directory (/Users/weimu/vibe/data/VOCdevkit/VOC2007/Annotations): Exists - Contains 9963 files\n",
      "Semantic segmentation directory (/Users/weimu/vibe/data/VOCdevkit/VOC2007/SegmentationClass): Exists - Contains 632 files\n",
      "Instance segmentation directory (/Users/weimu/vibe/data/VOCdevkit/VOC2007/SegmentationObject): Exists - Contains 632 files\n",
      "Image sets directory (/Users/weimu/vibe/data/VOCdevkit/VOC2007/ImageSets): Exists - Contains 4 files\n",
      "Training set contains 2501 images\n",
      "Validation set contains 2510 images\n",
      "Testing set contains 4952 images\n"
     ]
    }
   ],
   "source": [
    "# Check dataset structure\n",
    "def check_dataset_structure():\n",
    "    dirs = [\n",
    "        (IMAGE_DIR, 'Image directory'),\n",
    "        (ANNOTATION_DIR, 'Annotation directory'),\n",
    "        (SEGMENTATION_DIR, 'Semantic segmentation directory'),\n",
    "        (SEGMENTATION_INSTANCE_DIR, 'Instance segmentation directory'),\n",
    "        (IMAGESETS_DIR, 'Image sets directory')\n",
    "    ]\n",
    "\n",
    "    for dir_path, dir_name in dirs:\n",
    "        if os.path.exists(dir_path):\n",
    "            num_files = len(os.listdir(dir_path))\n",
    "            print(f\"{dir_name} ({dir_path}): Exists - Contains {num_files} files\")\n",
    "        else:\n",
    "            print(f\"{dir_name} ({dir_path}): Does not exist\")\n",
    "\n",
    "check_dataset_structure()\n",
    "\n",
    "# Get training and validation set image IDs\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "test_ids = []\n",
    "\n",
    "if os.path.exists(os.path.join(IMAGESETS_DIR, 'Main', 'train.txt')):\n",
    "    with open(os.path.join(IMAGESETS_DIR, 'Main', 'train.txt'), 'r') as f:\n",
    "        train_ids = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Training set contains {len(train_ids)} images\")\n",
    "\n",
    "if os.path.exists(os.path.join(IMAGESETS_DIR, 'Main', 'val.txt')):\n",
    "    with open(os.path.join(IMAGESETS_DIR, 'Main', 'val.txt'), 'r') as f:\n",
    "        val_ids = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Validation set contains {len(val_ids)} images\")\n",
    "\n",
    "if os.path.exists(os.path.join(IMAGESETS_DIR, 'Main', 'test.txt')):\n",
    "    with open(os.path.join(IMAGESETS_DIR, 'Main', 'test.txt'), 'r') as f:\n",
    "        test_ids = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Testing set contains {len(test_ids)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to parse XML annotation files\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"Parse VOC annotation XML file\"\"\"\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Get image information\n",
    "    size = root.find('size')\n",
    "    width = int(size.find('width').text)\n",
    "    height = int(size.find('height').text)\n",
    "\n",
    "    # Get all objects\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        difficult = int(obj.find('difficult').text) if obj.find('difficult') is not None else 0\n",
    "\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(float(bbox.find('xmin').text))\n",
    "        ymin = int(float(bbox.find('ymin').text))\n",
    "        xmax = int(float(bbox.find('xmax').text))\n",
    "        ymax = int(float(bbox.find('ymax').text))\n",
    "\n",
    "        objects.append({\n",
    "            'name': name,\n",
    "            'difficult': difficult,\n",
    "            'bbox': [xmin, ymin, xmax, ymax]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'objects': objects\n",
    "    }\n",
    "\n",
    "# Test parsing functionality\n",
    "if train_ids:\n",
    "    sample_id = train_ids[0]\n",
    "    sample_annotation_path = os.path.join(ANNOTATION_DIR, f\"{sample_id}.xml\")\n",
    "    if os.path.exists(sample_annotation_path):\n",
    "        sample_annotation = parse_annotation(sample_annotation_path)\n",
    "        print(f\"Sample image {sample_id} annotation:\")\n",
    "        print(f\"Size: {sample_annotation['width']} x {sample_annotation['height']}\")\n",
    "        print(f\"Number of objects: {len(sample_annotation['objects'])}\")\n",
    "        for i, obj in enumerate(sample_annotation['objects']):\n",
    "            print(f\"  Object {i+1}: {obj['name']}, Difficulty: {obj['difficult']}, Bounding box: {obj['bbox']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in the training set\n",
    "def analyze_class_distribution(image_ids):\n",
    "    class_counter = Counter()\n",
    "    difficult_counter = Counter()\n",
    "    total_objects = 0\n",
    "\n",
    "    for img_id in tqdm(image_ids, desc=\"Analyzing class distribution\"):\n",
    "        annotation_path = os.path.join(ANNOTATION_DIR, f\"{img_id}.xml\")\n",
    "        if os.path.exists(annotation_path):\n",
    "            annotation = parse_annotation(annotation_path)\n",
    "            for obj in annotation['objects']:\n",
    "                class_counter[obj['name']] += 1\n",
    "                if obj['difficult']:\n",
    "                    difficult_counter[obj['name']] += 1\n",
    "                total_objects += 1\n",
    "\n",
    "    return class_counter, difficult_counter, total_objects\n",
    "\n",
    "# Analyze training set\n",
    "if train_ids:\n",
    "    print(\"Analyzing training set class distribution...\")\n",
    "    train_class_counter, train_difficult_counter, train_total_objects = analyze_class_distribution(train_ids)\n",
    "\n",
    "    # Create class distribution table\n",
    "    class_data = []\n",
    "    for class_name, count in train_class_counter.most_common():\n",
    "        difficult_count = train_difficult_counter[class_name]\n",
    "        normal_count = count - difficult_count\n",
    "        class_data.append({\n",
    "            'Class': class_name,\n",
    "            'Total Count': count,\n",
    "            'Normal Samples': normal_count,\n",
    "            'Difficult Samples': difficult_count,\n",
    "            'Percentage (%)': round(count / train_total_objects * 100, 2)\n",
    "        })\n",
    "\n",
    "    class_df = pd.DataFrame(class_data)\n",
    "    display(class_df)\n",
    "\n",
    "    # Plot class distribution bar chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(class_df['Class'], class_df['Total Count'], color='skyblue')\n",
    "    plt.title('VOC2007 Training Set Class Distribution', fontsize=16)\n",
    "    plt.xlabel('Class', fontsize=14)\n",
    "    plt.ylabel('Sample Count', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    for i, count in enumerate(class_df['Total Count']):\n",
    "        plt.text(i, count + 50, str(count), ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bounding box size distribution\n",
    "def analyze_bbox_sizes(image_ids):\n",
    "    bbox_areas = []\n",
    "    bbox_aspect_ratios = []\n",
    "    class_bbox_areas = {}\n",
    "\n",
    "    for img_id in tqdm(image_ids, desc=\"Analyzing bounding box sizes\"):\n",
    "        annotation_path = os.path.join(ANNOTATION_DIR, f\"{img_id}.xml\")\n",
    "        if os.path.exists(annotation_path):\n",
    "            annotation = parse_annotation(annotation_path)\n",
    "            for obj in annotation['objects']:\n",
    "                bbox = obj['bbox']\n",
    "                width = bbox[2] - bbox[0]\n",
    "                height = bbox[3] - bbox[1]\n",
    "                area = width * height\n",
    "                aspect_ratio = width / height if height > 0 else 0\n",
    "\n",
    "                bbox_areas.append(area)\n",
    "                bbox_aspect_ratios.append(aspect_ratio)\n",
    "\n",
    "                class_name = obj['name']\n",
    "                if class_name not in class_bbox_areas:\n",
    "                    class_bbox_areas[class_name] = []\n",
    "                class_bbox_areas[class_name].append(area)\n",
    "\n",
    "    return bbox_areas, bbox_aspect_ratios, class_bbox_areas\n",
    "\n",
    "# Analyze bounding box sizes\n",
    "if train_ids:\n",
    "    print(\"Analyzing training set bounding box size distribution...\")\n",
    "    bbox_areas, bbox_aspect_ratios, class_bbox_areas = analyze_bbox_sizes(train_ids)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Plot bounding box area distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(bbox_areas, bins=50, alpha=0.7, color='blue')\n",
    "    plt.title('Bounding Box Area Distribution', fontsize=14)\n",
    "    plt.xlabel('Area (pixels)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Plot bounding box aspect ratio distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(bbox_aspect_ratios, bins=50, range=(0, 5), alpha=0.7, color='green')\n",
    "    plt.title('Bounding Box Aspect Ratio Distribution', fontsize=14)\n",
    "    plt.xlabel('Aspect Ratio (width/height)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot boxplot of bounding box areas by class\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    boxplot_data = [areas for cls, areas in class_bbox_areas.items()]\n",
    "    plt.boxplot(boxplot_data, labels=class_bbox_areas.keys())\n",
    "    plt.title('Bounding Box Area Distribution by Class', fontsize=16)\n",
    "    plt.xlabel('Class', fontsize=14)\n",
    "    plt.ylabel('Area (pixels)', fontsize=14)\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images with bounding boxes\n",
    "def visualize_samples(image_ids, num_samples=5):\n",
    "    \"\"\"Visualize sample images and bounding boxes\"\"\"\n",
    "    # Select random samples\n",
    "    sample_ids = random.sample(image_ids, min(num_samples, len(image_ids)))\n",
    "\n",
    "    # VOC dataset class colors\n",
    "    colors = {\n",
    "        'aeroplane': (255, 0, 0),      # Red\n",
    "        'bicycle': (0, 255, 0),        # Green\n",
    "        'bird': (0, 0, 255),           # Blue\n",
    "        'boat': (255, 255, 0),         # Yellow\n",
    "        'bottle': (255, 0, 255),       # Magenta\n",
    "        'bus': (0, 255, 255),          # Cyan\n",
    "        'car': (128, 0, 0),            # Dark red\n",
    "        'cat': (0, 128, 0),            # Dark green\n",
    "        'chair': (0, 0, 128),          # Dark blue\n",
    "        'cow': (128, 128, 0),          # Olive\n",
    "        'diningtable': (128, 0, 128),  # Purple\n",
    "        'dog': (0, 128, 128),          # Teal\n",
    "        'horse': (192, 0, 0),          # Brown-red\n",
    "        'motorbike': (0, 192, 0),      # Forest green\n",
    "        'person': (0, 0, 192),         # Navy blue\n",
    "        'pottedplant': (192, 192, 0),  # Dark yellow\n",
    "        'sheep': (192, 0, 192),        # Violet\n",
    "        'sofa': (0, 192, 192),         # Light teal\n",
    "        'train': (64, 64, 64),         # Dark gray\n",
    "        'tvmonitor': (192, 192, 192)   # Silver\n",
    "    }\n",
    "\n",
    "    # Draw sample images\n",
    "    _, axes = plt.subplots(num_samples, 1, figsize=(12, 6*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, img_id in enumerate(sample_ids):\n",
    "        img_path = os.path.join(IMAGE_DIR, f\"{img_id}.jpg\")\n",
    "        annotation_path = os.path.join(ANNOTATION_DIR, f\"{img_id}.xml\")\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(annotation_path):\n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Parse annotation\n",
    "            annotation = parse_annotation(annotation_path)\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for obj in annotation['objects']:\n",
    "                bbox = obj['bbox']\n",
    "                class_name = obj['name']\n",
    "                xmin, ymin, xmax, ymax = bbox\n",
    "\n",
    "                # Get class color, default to gray\n",
    "                color = colors.get(class_name, (128, 128, 128))\n",
    "\n",
    "                # Convert BGR to RGB\n",
    "                color_rgb = (color[0], color[1], color[2])\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color_rgb, 2)\n",
    "\n",
    "                # Add class label\n",
    "                label = f\"{class_name}\"\n",
    "                font_scale = 0.7\n",
    "                font_thickness = 2\n",
    "                (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n",
    "\n",
    "                # Background rectangle\n",
    "                cv2.rectangle(img, (xmin, ymin - text_height - 10), (xmin + text_width + 10, ymin), color_rgb, -1)\n",
    "\n",
    "                # Text\n",
    "                cv2.putText(img, label, (xmin + 5, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            font_scale, (255, 255, 255), font_thickness)\n",
    "\n",
    "            # Show image\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"Image ID: {img_id} ({annotation['width']}x{annotation['height']})\")\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f\"Image or annotation not found: {img_id}\", ha='center', va='center')\n",
    "            axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize random samples from training set\n",
    "if train_ids:\n",
    "    print(\"Visualizing random training samples...\")\n",
    "    visualize_samples(train_ids, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze segmentation masks (if available)\n",
    "def visualize_segmentation(image_ids, num_samples=3):\n",
    "    \"\"\"Visualize semantic and instance segmentation masks\"\"\"\n",
    "    # Select random samples with segmentation masks\n",
    "    sample_ids = []\n",
    "    for img_id in random.sample(image_ids, min(len(image_ids), num_samples*3)):\n",
    "        seg_path = os.path.join(SEGMENTATION_DIR, f\"{img_id}.png\")\n",
    "        inst_path = os.path.join(SEGMENTATION_INSTANCE_DIR, f\"{img_id}.png\")\n",
    "        if os.path.exists(seg_path) and os.path.exists(inst_path):\n",
    "            sample_ids.append(img_id)\n",
    "            if len(sample_ids) >= num_samples:\n",
    "                break\n",
    "\n",
    "    if not sample_ids:\n",
    "        print(\"Could not find samples with both semantic and instance segmentation masks.\")\n",
    "        return\n",
    "\n",
    "    # Create visualization for each sample\n",
    "    for img_id in sample_ids:\n",
    "        img_path = os.path.join(IMAGE_DIR, f\"{img_id}.jpg\")\n",
    "        seg_path = os.path.join(SEGMENTATION_DIR, f\"{img_id}.png\")\n",
    "        inst_path = os.path.join(SEGMENTATION_INSTANCE_DIR, f\"{img_id}.png\")\n",
    "\n",
    "        # Read image and masks\n",
    "        img = np.array(Image.open(img_path))\n",
    "        seg_mask = np.array(Image.open(seg_path))\n",
    "        inst_mask = np.array(Image.open(inst_path))\n",
    "\n",
    "        # Create image grid\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "        # Original image\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(f\"Original Image: {img_id}\", fontsize=14)\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Semantic segmentation mask\n",
    "        axes[1].imshow(seg_mask)\n",
    "        axes[1].set_title(\"Semantic Segmentation Mask\", fontsize=14)\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # Instance segmentation mask\n",
    "        axes[2].imshow(inst_mask)\n",
    "        axes[2].set_title(\"Instance Segmentation Mask\", fontsize=14)\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize segmentation masks\n",
    "if train_ids:\n",
    "    print(\"Visualizing segmentation mask examples...\")\n",
    "    visualize_segmentation(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze number of objects per image\n",
    "def analyze_objects_per_image(image_ids):\n",
    "    \"\"\"Analyze number of objects per image\"\"\"\n",
    "    objects_per_image = []\n",
    "    class_count_per_image = {}\n",
    "\n",
    "    for img_id in tqdm(image_ids, desc=\"Analyzing objects per image\"):\n",
    "        annotation_path = os.path.join(ANNOTATION_DIR, f\"{img_id}.xml\")\n",
    "        if os.path.exists(annotation_path):\n",
    "            annotation = parse_annotation(annotation_path)\n",
    "\n",
    "            # Count total objects\n",
    "            num_objects = len(annotation['objects'])\n",
    "            objects_per_image.append(num_objects)\n",
    "\n",
    "            # Count objects by class\n",
    "            class_counts = Counter([obj['name'] for obj in annotation['objects']])\n",
    "            for cls, count in class_counts.items():\n",
    "                if cls not in class_count_per_image:\n",
    "                    class_count_per_image[cls] = []\n",
    "                class_count_per_image[cls].append(count)\n",
    "\n",
    "    return objects_per_image, class_count_per_image\n",
    "\n",
    "# Analyze objects per image\n",
    "if train_ids:\n",
    "    print(\"Analyzing objects per image in training set...\")\n",
    "    objects_per_image, class_count_per_image = analyze_objects_per_image(train_ids)\n",
    "\n",
    "    # Plot distribution of objects per image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(objects_per_image, bins=range(0, max(objects_per_image) + 2), alpha=0.7, color='blue')\n",
    "    plt.title('Objects per Image Distribution', fontsize=16)\n",
    "    plt.xlabel('Number of Objects', fontsize=14)\n",
    "    plt.ylabel('Number of Images', fontsize=14)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(range(0, max(objects_per_image) + 1))\n",
    "\n",
    "    avg_objects = sum(objects_per_image) / len(objects_per_image) if objects_per_image else 0\n",
    "    plt.axvline(x=avg_objects, color='red', linestyle='--')\n",
    "    plt.text(avg_objects + 0.1, plt.ylim()[1] * 0.9, f'Average: {avg_objects:.2f}', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Count images with multiple classes\n",
    "    multi_class_images = sum(1 for counts in class_count_per_image.values() if sum(counts) > 0)\n",
    "    single_class_images = len(train_ids) - multi_class_images\n",
    "\n",
    "    # Plot pie chart showing single vs multi-class image ratio\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie([single_class_images, multi_class_images],\n",
    "            labels=['Single-class Images', 'Multi-class Images'],\n",
    "            autopct='%1.1f%%',\n",
    "            colors=['lightblue', 'lightgreen'],\n",
    "            startangle=90,\n",
    "            explode=(0, 0.1))\n",
    "    plt.title('Single vs Multi-class Images Ratio', fontsize=16)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize dataset features\n",
    "def summarize_dataset(train_ids, val_ids=None):\n",
    "    \"\"\"Summarize main features of the dataset\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"VOC2007 Dataset Summary\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Training set statistics\n",
    "    if train_ids:\n",
    "        train_images = len(train_ids)\n",
    "\n",
    "        # Parse all objects\n",
    "        train_objects = []\n",
    "        train_classes = set()\n",
    "        for img_id in tqdm(train_ids, desc=\"Summarizing training set\"):\n",
    "            annotation_path = os.path.join(ANNOTATION_DIR, f\"{img_id}.xml\")\n",
    "            if os.path.exists(annotation_path):\n",
    "                annotation = parse_annotation(annotation_path)\n",
    "                train_objects.extend(annotation['objects'])\n",
    "                train_classes.update([obj['name'] for obj in annotation['objects']])\n",
    "\n",
    "        # Statistics\n",
    "        print(f\"\\nTraining Set:\")\n",
    "        print(f\"- Number of images: {train_images}\")\n",
    "        print(f\"- Total objects: {len(train_objects)}\")\n",
    "        print(f\"- Number of classes: {len(train_classes)}\")\n",
    "        print(f\"- Average objects per image: {len(train_objects) / train_images:.2f}\")\n",
    "\n",
    "        # Class list\n",
    "        print(f\"- Class list: {', '.join(sorted(train_classes))}\")\n",
    "\n",
    "    # Validation set statistics\n",
    "    if val_ids:\n",
    "        val_images = len(val_ids)\n",
    "\n",
    "        # Parse all objects\n",
    "        val_objects = []\n",
    "        val_classes = set()\n",
    "        for img_id in tqdm(val_ids, desc=\"Summarizing validation set\"):\n",
    "            annotation_path = os.path.join(ANNOTATION_DIR, f\"{img_id}.xml\")\n",
    "            if os.path.exists(annotation_path):\n",
    "                annotation = parse_annotation(annotation_path)\n",
    "                val_objects.extend(annotation['objects'])\n",
    "                val_classes.update([obj['name'] for obj in annotation['objects']])\n",
    "\n",
    "        # Statistics\n",
    "        print(f\"\\nValidation Set:\")\n",
    "        print(f\"- Number of images: {val_images}\")\n",
    "        print(f\"- Total objects: {len(val_objects)}\")\n",
    "        print(f\"- Number of classes: {len(val_classes)}\")\n",
    "        print(f\"- Average objects per image: {len(val_objects) / val_images:.2f}\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"- VOC2007 dataset is relatively balanced, but 'person' class typically has higher proportion\")\n",
    "    print(\"- Dataset includes samples of varying difficulty, with 'difficult=1' samples potentially ignored in evaluation\")\n",
    "    print(\"- Most common object size range is medium-sized, but there's significant size variation\")\n",
    "    print(\"- Some images contain objects from multiple classes, adding to detection complexity\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Generate dataset summary\n",
    "if train_ids:\n",
    "    summarize_dataset(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vbl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
