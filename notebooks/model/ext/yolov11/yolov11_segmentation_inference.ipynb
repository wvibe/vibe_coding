{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4eaa2a",
   "metadata": {},
   "source": [
    "# YOLOv11 Segmentation Inference & Basic Comparison\n",
    "\n",
    "This notebook demonstrates basic inference using a pre-trained YOLOv11 segmentation model,\n",
    "loads data from the VOC dataset, stores results systematically, and performs simple\n",
    "quantitative and visual comparisons between predictions and ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16470485",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7b2d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import cv2  # Make sure opencv-python is installed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Make sure numpy is installed\n",
    "from dotenv import load_dotenv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Ensure plots are displayed inline in notebooks if run there\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5904d2",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b415f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: yolo11n-seg.pt\n"
     ]
    }
   ],
   "source": [
    "model_name = \"yolo11n-seg.pt\"\n",
    "model = YOLO(model_name)\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "\n",
    "# Define class names for VOC (adjust if using a different dataset)\n",
    "# Based on standard VOC order\n",
    "VOC_CLASS_NAMES = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]\n",
    "\n",
    "# Define colors for visualization (20 classes)\n",
    "# Using tab20 colormap for distinct colors\n",
    "COLORS = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "# Convert to BGR for OpenCV (0-255)\n",
    "CV_COLORS = [(int(c[2] * 255), int(c[1] * 255), int(c[0] * 255)) for c in COLORS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa2288",
   "metadata": {},
   "source": [
    "## 3. Prepare Data from VOC Dataset\n",
    "\n",
    "Load images and corresponding ground truth label paths from the specified VOC split.\n",
    "Store paths and results in a dictionary keyed by image basename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0111d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC segment is at /home/ubuntu/vibe/hub/datasets/VOC/segment\n",
      "Prepared 5 images\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "split = \"val2007\"  # Options: 'train2007', 'val2007', 'train2012', 'val2012'\n",
    "num_samples = 5  # Number of random images to process\n",
    "\n",
    "# Load .env file to get dataset path\n",
    "project_root_for_env = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), \"../../..\")\n",
    ")  # Adjust depth if needed\n",
    "dotenv_path = os.path.join(project_root_for_env, \".env\")\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    # print(f\"Loaded environment variables from: {dotenv_path}\")\n",
    "\n",
    "VOC_SEGMENT_PATH = os.getenv(\"VOC_SEGMENT\")\n",
    "print(f\"VOC segment is at {VOC_SEGMENT_PATH}\")\n",
    "\n",
    "# Prepare image and label files\n",
    "image_dir = os.path.join(VOC_SEGMENT_PATH, \"images\", split)\n",
    "label_dir = os.path.join(VOC_SEGMENT_PATH, \"labels\", split)\n",
    "assert os.path.isdir(image_dir) and os.path.isdir(label_dir)\n",
    "\n",
    "all_image_files = [\n",
    "    f for f in os.listdir(image_dir) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "]\n",
    "if len(all_image_files) >= num_samples:\n",
    "    selected_image_files = random.sample(all_image_files, num_samples)\n",
    "else:\n",
    "    selected_image_files = all_image_files\n",
    "print(f\"Prepared {len(selected_image_files)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be9af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data for 5 images from VOC 'val2007' split:\n",
      "  1. ID: 008307, Image: /home/ubuntu/vibe/hub/datasets/VOC/segment/images/val2007/008307.jpg, Label: /home/ubuntu/vibe/hub/datasets/VOC/segment/labels/val2007/008307.txt\n",
      "  2. ID: 003137, Image: /home/ubuntu/vibe/hub/datasets/VOC/segment/images/val2007/003137.jpg, Label: /home/ubuntu/vibe/hub/datasets/VOC/segment/labels/val2007/003137.txt\n",
      "  3. ID: 006281, Image: /home/ubuntu/vibe/hub/datasets/VOC/segment/images/val2007/006281.jpg, Label: /home/ubuntu/vibe/hub/datasets/VOC/segment/labels/val2007/006281.txt\n",
      "  4. ID: 007021, Image: /home/ubuntu/vibe/hub/datasets/VOC/segment/images/val2007/007021.jpg, Label: /home/ubuntu/vibe/hub/datasets/VOC/segment/labels/val2007/007021.txt\n",
      "  5. ID: 007165, Image: /home/ubuntu/vibe/hub/datasets/VOC/segment/images/val2007/007165.jpg, Label: /home/ubuntu/vibe/hub/datasets/VOC/segment/labels/val2007/007165.txt\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dictionary\n",
    "data_dict = {}\n",
    "for img_file in selected_image_files:\n",
    "    basename = os.path.splitext(img_file)[0]\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    label_path = os.path.join(label_dir, basename + \".txt\")\n",
    "\n",
    "    if os.path.exists(img_path) and os.path.exists(label_path):\n",
    "        data_dict[basename] = {\n",
    "            \"image_path\": img_path,\n",
    "            \"label_path\": label_path,\n",
    "            \"prediction\": None,  # Placeholder for results\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Skipping {basename}, missing file.\") # Keep it cleaner for notebook\n",
    "\n",
    "print(f\"Prepared data for {len(data_dict)} images from VOC '{split}' split:\")\n",
    "for i, (basename, data) in enumerate(data_dict.items()):\n",
    "    print(\n",
    "        f\"  {i + 1}. ID: {basename}, Image: {data['image_path']}, Label: {data['label_path']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65783ffa",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "Run YOLOv11 segmentation prediction on the selected images and store results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79edc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running prediction on 5 images...\n",
      "\n",
      "0: 640x640 1 person, 2 horses, 18.0ms\n",
      "1: 640x640 1 bus, 18.0ms\n",
      "2: 640x640 3 boats, 18.0ms\n",
      "3: 640x640 2 bottles, 7 bowls, 6 oranges, 4 chairs, 1 potted plant, 1 dining table, 1 tv, 18.0ms\n",
      "4: 640x640 1 sheep, 18.0ms\n",
      "Speed: 3.8ms preprocess, 18.0ms inference, 111.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Total predictions: 5, Missed: 0\n",
      "\n",
      "\n",
      "Display prediction for: 008307\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: ultralytics.engine.results.Masks object\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[206, 196, 166],\n",
      "        [206, 196, 166],\n",
      "        [206, 196, 166],\n",
      "        ...,\n",
      "        [ 97, 107, 101],\n",
      "        [ 98, 107, 104],\n",
      "        [103, 112, 109]],\n",
      "\n",
      "       [[206, 196, 166],\n",
      "        [206, 196, 166],\n",
      "        [206, 196, 166],\n",
      "        ...,\n",
      "        [106, 118, 112],\n",
      "        [108, 119, 116],\n",
      "        [107, 118, 115]],\n",
      "\n",
      "       [[206, 196, 166],\n",
      "        [206, 196, 166],\n",
      "        [206, 196, 166],\n",
      "        ...,\n",
      "        [118, 133, 129],\n",
      "        [118, 135, 132],\n",
      "        [113, 130, 127]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 78, 155, 148],\n",
      "        [ 74, 151, 144],\n",
      "        [ 69, 146, 139],\n",
      "        ...,\n",
      "        [116, 103,  71],\n",
      "        [114, 101,  69],\n",
      "        [114, 101,  69]],\n",
      "\n",
      "       [[ 67, 145, 138],\n",
      "        [ 65, 143, 136],\n",
      "        [ 59, 137, 130],\n",
      "        ...,\n",
      "        [114, 102,  66],\n",
      "        [112, 100,  66],\n",
      "        [113, 101,  65]],\n",
      "\n",
      "       [[ 75, 155, 148],\n",
      "        [ 59, 139, 132],\n",
      "        [ 76, 156, 149],\n",
      "        ...,\n",
      "        [116, 103,  65],\n",
      "        [114, 100,  64],\n",
      "        [115, 102,  64]]], dtype=uint8)\n",
      "orig_shape: (367, 480)\n",
      "path: '/home/ubuntu/vibe/hub/datasets/VOC/segment/images/val2007/008307.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/segment/predict'\n",
      "speed: {'preprocess': 3.774389799218625, 'inference': 18.040258801192977, 'postprocess': 111.07003439974505}\n"
     ]
    }
   ],
   "source": [
    "image_paths_to_predict = [data[\"image_path\"] for data in data_dict.values()]\n",
    "print(f\"\\nRunning prediction on {len(image_paths_to_predict)} images...\")\n",
    "\n",
    "results = model.predict(\n",
    "    source=image_paths_to_predict, save=False, save_txt=False\n",
    ")  # Don't save automatically now\n",
    "\n",
    "pred_cnt, miss_cnt = 0, 0\n",
    "# Store results back into the dictionary, assuming order is preserved\n",
    "basenames = list(data_dict.keys())\n",
    "for i, result in enumerate(results):\n",
    "    basename = os.path.splitext(os.path.basename(result.path))[0]\n",
    "    if basename in data_dict:\n",
    "        data_dict[basename][\"prediction\"] = result\n",
    "        pred_cnt += 1\n",
    "    else: # Fallback if basenames don't match somehow (shouldn't happen)\n",
    "        print(f\"Warning: Basename {basename} not found in data_dict. Skipping result.\")\n",
    "        miss_cnt += 1\n",
    "\n",
    "print(f\"\\nTotal predictions: {pred_cnt}, Missed: {miss_cnt}\\n\")\n",
    "\n",
    "# Example: Access prediction for the first image\n",
    "first_key = list(data_dict.keys())[0]\n",
    "print(f\"\\nDisplay prediction for: {first_key}\")\n",
    "print(data_dict[first_key]['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cac19b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 5. Quantitative Comparison (Object Counts)\n",
    "\n",
    "Compare the number of detected objects per class in the prediction vs. the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_yolo_segment_label_counts(label_path):\n",
    "    \"\"\"Parses a YOLO segmentation label file and returns class counts.\"\"\"\n",
    "    counts = Counter()\n",
    "    if not os.path.exists(label_path):\n",
    "        return counts\n",
    "    try:\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) > 1:\n",
    "                    class_id = int(parts[0])\n",
    "                    counts[class_id] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing label file {label_path}: {e}\")\n",
    "    return counts\n",
    "\n",
    "\n",
    "print(\"\\n--- Quantitative Comparison (Class Counts) ---\")\n",
    "\n",
    "for basename, data in data_dict.items():\n",
    "    print(f\"\\nImage ID: {basename}\")\n",
    "\n",
    "    lbl_counts = parse_yolo_segment_label_counts(data[\"label_path\"])\n",
    "    lbl_counts_named = {\n",
    "        VOC_CLASS_NAMES[cid]: count\n",
    "        for cid, count in lbl_counts.items()\n",
    "        if cid < len(VOC_CLASS_NAMES)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    prediction = data.get(\"prediction\")\n",
    "    assert prediction and prediction.boxes and prediction.names\n",
    "\n",
    "    pred_counts = Counter()\n",
    "    pred_class_indices = prediction.boxes.cls.int().tolist()\n",
    "    pred_class_names = [prediction.names[i] for i in pred_class_indices]\n",
    "    pred_counts.update(pred_class_names)\n",
    "\n",
    "    print(f\"  Labeled: {lbl_counts_named}\")\n",
    "    print(f\"  Predicted: {dict(pred_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec896b86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 6. Visual Comparison (Segmentation Masks)\n",
    "\n",
    "Visualize the original image, the ground truth segmentation, and the predicted segmentation side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b08941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_gt_segmentation(image_path, label_path):\n",
    "    \"\"\"Reads image and label file, draws GT segmentation polygons.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error reading image: {image_path}\")\n",
    "        return None\n",
    "    h, w = img.shape[:2]\n",
    "    img_draw = img.copy()\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        return img_draw  # Return original image if no label file\n",
    "\n",
    "    try:\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 7:  # class_id + at least 3 points (x,y pairs)\n",
    "                    class_id = int(parts[0])\n",
    "                    poly_norm = np.array(parts[1:], dtype=float).reshape(-1, 2)\n",
    "                    poly_denorm = (poly_norm * np.array([w, h])).astype(int)\n",
    "\n",
    "                    color = CV_COLORS[\n",
    "                        class_id % len(CV_COLORS)\n",
    "                    ]  # Cycle through colors if more classes than defined\n",
    "                    # Draw filled polygon (optional, can be visually noisy)\n",
    "                    # cv2.fillPoly(img_draw, [poly_denorm], color)\n",
    "                    # Draw polygon outline\n",
    "                    cv2.polylines(img_draw, [poly_denorm], isClosed=True, color=color, thickness=2)\n",
    "                    # Put class name text (optional)\n",
    "                    # label_pos = tuple(poly_denorm.min(axis=0))\n",
    "                    # cv2.putText(img_draw, VOC_CLASS_NAMES[class_id], label_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error drawing GT for {label_path}: {e}\")\n",
    "\n",
    "    return img_draw\n",
    "\n",
    "\n",
    "print(\"\\n--- Visual Comparison --- (GT vs. Prediction)\")\n",
    "\n",
    "for basename, data in data_dict.items():\n",
    "    print(f\"\\nVisualizing: {basename}\")\n",
    "    img_path = data[\"image_path\"]\n",
    "    label_path = data[\"label_path\"]\n",
    "    prediction = data.get(\"prediction\")\n",
    "\n",
    "    # Read original image\n",
    "    img_original = cv2.imread(img_path)\n",
    "    if img_original is None:\n",
    "        continue\n",
    "    img_original_rgb = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Generate GT visualization\n",
    "    img_gt = draw_gt_segmentation(img_path, label_path)\n",
    "    img_gt_rgb = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB) if img_gt is not None else img_original_rgb\n",
    "\n",
    "    # Generate prediction visualization\n",
    "    if prediction:\n",
    "        img_pred_array = prediction.plot()  # Returns annotated image as BGR numpy array\n",
    "        img_pred_rgb = cv2.cvtColor(img_pred_array, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        img_pred_rgb = np.copy(img_original_rgb)\n",
    "        cv2.putText(\n",
    "            img_pred_rgb, \"No Prediction\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2\n",
    "        )\n",
    "\n",
    "    # Plot side-by-side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    fig.suptitle(f\"Comparison for: {basename}\", fontsize=16)\n",
    "\n",
    "    axes[0].imshow(img_original_rgb)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(img_gt_rgb)\n",
    "    axes[1].set_title(\"Ground Truth Segmentation\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(img_pred_rgb)\n",
    "    axes[2].set_title(\"Predicted Segmentation\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent title overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97258558",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook demonstrated loading a pre-trained YOLOv11 segmentation model, running inference on data from the VOC dataset, and performing basic quantitative (class counts) and visual (side-by-side masks) comparisons against ground truth labels."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
