{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94db7ddc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# YOLOv8 Introduction\n",
    "\n",
    "This notebook demonstrates how to use YOLOv8 models from Ultralytics for object detection tasks. We will cover:\n",
    "\n",
    "1. **Inference** - Using pre-trained models to detect objects in images\n",
    "2. **Fine-tuning** - Adapting a pre-trained model to our PASCAL VOC dataset\n",
    "3. **Training from scratch** - Explaining the process of creating a new model trained on our dataset\n",
    "\n",
    "The PASCAL VOC dataset contains 20 object categories including person, car, dog, etc. We'll be using the VOC2007 and VOC2012 datasets which are already downloaded and located at the path specified in the `VOC_ROOT` environment variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f1fc2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and set up our environment. We need to ensure the `VOC_ROOT` environment variable is loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16294b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ultralytics version\n",
    "!pip show ultralytics | grep Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738afdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/wmu/vibe/vibe_coding\n",
      ".env loaded: True\n",
      "VOC dataset path: /home/wmu/vibe/hub/datasets/VOC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get project root directory\n",
    "project_root = Path().absolute().parents[3]  # Assumes notebook is in notebooks/model/ext/yolov8\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add project root to sys.path if needed (optional, depends on execution context)\n",
    "# if str(project_root) not in sys.path:\n",
    "#     sys.path.append(str(project_root))\n",
    "\n",
    "# Load environment variables from .env file at the project root\n",
    "dotenv_path = project_root / \".env\"\n",
    "loaded = load_dotenv(dotenv_path=dotenv_path)\n",
    "print(f\".env loaded: {loaded}\")\n",
    "\n",
    "# Access VOC_ROOT environment variable\n",
    "VOC_ROOT = os.environ.get(\"VOC_ROOT\")\n",
    "if VOC_ROOT:\n",
    "    print(f\"VOC dataset path: {VOC_ROOT}\")\n",
    "    # Check if the directory exists\n",
    "    if not Path(VOC_ROOT).is_dir():\n",
    "        print(f\"Warning: VOC_ROOT directory '{VOC_ROOT}' does not exist! Please check .env file.\")\n",
    "else:\n",
    "    print(\"Error: VOC_ROOT environment variable not set. Please check .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9704e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Inference with Pre-trained YOLOv8 Models\n",
    "\n",
    "YOLOv8 comes with several pre-trained models. Let's load `yolo8n.pt` (YOLOv8 Nano) and run inference on a random image from the VOC dataset.\n",
    "\n",
    "_Note: The first time you use a model like `yolo8n.pt`, Ultralytics will automatically download it._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b695f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained YOLOv8 nano model\n",
    "try:\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    print(f\"Model yolo8n.pt loaded successfully.\")\n",
    "    # Print model class names (should be COCO classes initially)\n",
    "    # print(\"Default model classes:\", model.names)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a651c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a random image path from the VOC dataset images/test2007 folder\n",
    "# Ensure necessary imports are available (usually done in earlier cells, but good practice)\n",
    "from pathlib import Path\n",
    "\n",
    "def load_random_voc_image(voc_root_path):\n",
    "    \"\"\"Loads a random image path from the VOC dataset's images/test2007 directory.\"\"\"\n",
    "    if not voc_root_path or not Path(voc_root_path).is_dir():\n",
    "        print(\"VOC_ROOT path is invalid or not set.\")\n",
    "        return None\n",
    "\n",
    "    # Target the test2007 directory within images/\n",
    "    test_images_dir = Path(voc_root_path) / 'images' / 'test2007'\n",
    "    if not test_images_dir.is_dir():\n",
    "        print(f\"Test images directory not found at {test_images_dir}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Find all JPEG images in the directory using Path.glob\n",
    "        image_files = list(test_images_dir.glob('*.jpg'))\n",
    "        # Alternative using glob module:\n",
    "        # image_files = glob.glob(str(test_images_dir / '*.jpg'))\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No .jpg images found in {test_images_dir}\")\n",
    "            return None\n",
    "\n",
    "        # Select a random image path\n",
    "        random_image_path = random.choice(image_files)\n",
    "\n",
    "        print(f\"Selected random image: {random_image_path}\")\n",
    "        return str(random_image_path) # Return as string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading random image from {test_images_dir}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get a random test image path using the updated function\n",
    "test_image_path = load_random_voc_image(VOC_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84002b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference if image path is valid\n",
    "if test_image_path and \"model\" in locals():\n",
    "    try:\n",
    "        results = model(test_image_path)\n",
    "\n",
    "        # Display the results\n",
    "        # results[0].show() # Opens a separate window\n",
    "\n",
    "        # Plot results within the notebook\n",
    "        res_plotted = results[0].plot()  # Returns annotated image as numpy array (BGR)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(\n",
    "            cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB)\n",
    "        )  # Convert BGR to RGB for matplotlib\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"YOLOv8 Inference Results\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference or plotting: {e}\")\n",
    "else:\n",
    "    print(\"Skipping inference due to missing image path or model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8150ec2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Examining Detection Results\n",
    "\n",
    "The `results` object contains detailed information about the detections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c97595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first result if available\n",
    "if \"results\" in locals() and results:\n",
    "    result = results[0]  # First image result\n",
    "\n",
    "    # Access bounding boxes (xyxy format), confidence scores, and class IDs\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()  # Bounding boxes: x1, y1, x2, y2\n",
    "    confidences = result.boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    class_ids = result.boxes.cls.cpu().numpy().astype(int)  # Class IDs\n",
    "\n",
    "    # Get class names from the model\n",
    "    class_names = result.names  # Use names from the result object directly\n",
    "\n",
    "    # Print detection details\n",
    "    print(f\"Found {len(boxes)} objects:\\n\")\n",
    "    for i, (box, conf, cls_id) in enumerate(zip(boxes, confidences, class_ids)):\n",
    "        cls_name = class_names[cls_id]\n",
    "        print(f\"Detection {i + 1}: Class='{cls_name}' ({cls_id}), Confidence={conf:.2f}, Box={box}\")\n",
    "else:\n",
    "    print(\"No results available to examine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380afef",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Fine-tuning YOLOv8 on PASCAL VOC Dataset\n",
    "\n",
    "Let's fine-tune the pre-trained `yolo8n.pt` model on our PASCAL VOC dataset. We will use the configuration file `src/models/ext/yolov8/configs/voc.yaml`.\n",
    "\n",
    "**Important:** Fine-tuning requires significant computational resources (GPU recommended) and time. For this demo, we'll run for only **1 epoch** with a **small image size** and **small batch size**. Adjust these parameters (`epochs`, `imgsz`, `batch`) for actual training based on your hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb4133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset config path: /home/wmu/vibe/vibe_coding/src/models/ext/yolov8/configs/voc2007.yaml\n"
     ]
    }
   ],
   "source": [
    "# Path to our VOC dataset configuration YAML\n",
    "voc_yaml_path = project_root / 'src/models/ext/yolov8/configs/voc2007.yaml'\n",
    "print(f\"Dataset config path: {voc_yaml_path}\")\n",
    "\n",
    "# Check if the YAML file exists\n",
    "if not voc_yaml_path.is_file():\n",
    "    print(f\"Error: Dataset YAML file not found at {voc_yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672c3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded yolo8n.pt for fine-tuning.\n",
      "Ultralytics 8.3.98 ðŸš€ Python-3.12.9 torch-2.6.0+cu124 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "                                                      CUDA:1 (NVIDIA RTX A5000, 24248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/wmu/vibe/vibe_coding/src/models/ext/yolov8/configs/voc2007.yaml, epochs=1, time=None, patience=100, batch=8, imgsz=320, save=True, save_period=-1, cache=False, device=0,1, workers=8, project=runs/detect, name=yolov8n_voc_finetune_demo, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolov8n_voc_finetune_demo\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/wmu/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 1.56MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mDDP:\u001b[0m debug command /opt/conda/envs/vbl/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 58161 /home/wmu/.config/Ultralytics/DDP/_temp_9ofttado140583338282544.py\n",
      "Ultralytics 8.3.98 ðŸš€ Python-3.12.9 torch-2.6.0+cu124 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "                                                      CUDA:1 (NVIDIA RTX A5000, 24248MiB)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/yolov8n_voc_finetune_demo', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: muwayne-vibe (muwayne-vibe-dailygpt) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.8\n",
      "wandb: Run data is saved locally in /home/wmu/vibe/vibe_coding/notebooks/model/ext/yolov8/wandb/run-20250329_140400-6la5c9wg\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run yolov8n_voc_finetune_demo\n",
      "wandb: â­ï¸ View project at https://wandb.ai/muwayne-vibe-dailygpt/runs-detect\n",
      "wandb: ðŸš€ View run at https://wandb.ai/muwayne-vibe-dailygpt/runs-detect/runs/6la5c9wg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=20\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:02<00:00, 2.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/wmu/vibe/hub/datasets/VOC/labels/train2007... 2501 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2501/2501 [00:01<00:00, 1471.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/wmu/vibe/hub/datasets/VOC/labels/train2007.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/wmu/vibe/hub/datasets/VOC/labels/val2007... 2510 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2510/2510 [00:02<00:00, 1143.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/wmu/vibe/hub/datasets/VOC/labels/val2007.cache\n",
      "Plotting labels to runs/detect/yolov8n_voc_finetune_demo/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/yolov8n_voc_finetune_demo\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1     0.283G      1.304      3.569      1.296         24        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:37<00:00,  8.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:16<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2510       7818      0.557      0.308      0.318      0.219\n",
      "\n",
      "1 epochs completed in 0.017 hours.\n",
      "Optimizer stripped from runs/detect/yolov8n_voc_finetune_demo/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/yolov8n_voc_finetune_demo/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/yolov8n_voc_finetune_demo/weights/best.pt...\n",
      "Ultralytics 8.3.98 ðŸš€ Python-3.12.9 torch-2.6.0+cu124 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "                                                      CUDA:1 (NVIDIA RTX A5000, 24248MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:11<00:00, 28.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2510       7818      0.561        0.3      0.318      0.219\n",
      "             aeroplane        127        175      0.429      0.099      0.234      0.169\n",
      "               bicycle        133        216      0.389      0.421      0.387      0.254\n",
      "                  bird        151        305      0.289       0.22      0.179      0.129\n",
      "                  boat        101        190          1          0     0.0127    0.00886\n",
      "                bottle        109        296          1          0     0.0678      0.043\n",
      "                   bus         97        141      0.408      0.355      0.345      0.309\n",
      "                   car        359        818      0.397      0.663      0.588      0.404\n",
      "                   cat        178        198      0.376      0.652      0.568       0.41\n",
      "                 chair        290        706      0.331      0.251      0.219      0.105\n",
      "                   cow         75        171          0          0      0.068     0.0477\n",
      "           diningtable        133        162      0.634     0.0111      0.335      0.213\n",
      "                   dog        220        267       0.32      0.277      0.245      0.175\n",
      "                 horse        150        199      0.581      0.705       0.66      0.462\n",
      "             motorbike        126        197      0.404      0.411      0.268      0.178\n",
      "                person       1025       2742      0.671      0.635      0.673      0.404\n",
      "           pottedplant        120        320          1          0     0.0241     0.0125\n",
      "                 sheep         48        162          1          0     0.0207     0.0144\n",
      "                  sofa        184        207      0.586      0.411      0.461      0.325\n",
      "                 train        135        170      0.706      0.529      0.626       0.48\n",
      "             tvmonitor        135        176      0.699      0.369      0.384      0.241\n",
      "Speed: 0.0ms preprocess, 0.8ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolov8n_voc_finetune_demo\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: uploading artifact run_6la5c9wg_model; uploading artifact run-6la5c9wg-curvesF1-ConfidenceB_table; uploading artifact run-6la5c9wg-curvesPrecision-ConfidenceB_table; uploading artifact run-6la5c9wg-curvesRecall-ConfidenceB_table\n",
      "wandb: uploading artifact run-6la5c9wg-curvesPrecision-ConfidenceB_table; uploading artifact run-6la5c9wg-curvesRecall-ConfidenceB_table\n",
      "wandb: uploading history steps 0-0, summary, console lines 55-56\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                  lr/pg0 â–\n",
      "wandb:                  lr/pg1 â–\n",
      "wandb:                  lr/pg2 â–\n",
      "wandb:        metrics/mAP50(B) â–\n",
      "wandb:     metrics/mAP50-95(B) â–\n",
      "wandb:    metrics/precision(B) â–\n",
      "wandb:       metrics/recall(B) â–\n",
      "wandb:            model/GFLOPs â–\n",
      "wandb:        model/parameters â–\n",
      "wandb: model/speed_PyTorch(ms) â–\n",
      "wandb:          train/box_loss â–\n",
      "wandb:          train/cls_loss â–\n",
      "wandb:          train/dfl_loss â–\n",
      "wandb:            val/box_loss â–\n",
      "wandb:            val/cls_loss â–\n",
      "wandb:            val/dfl_loss â–\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                  lr/pg0 0.00014\n",
      "wandb:                  lr/pg1 0.00014\n",
      "wandb:                  lr/pg2 0.00014\n",
      "wandb:        metrics/mAP50(B) 0.3183\n",
      "wandb:     metrics/mAP50-95(B) 0.21932\n",
      "wandb:    metrics/precision(B) 0.56104\n",
      "wandb:       metrics/recall(B) 0.3004\n",
      "wandb:            model/GFLOPs 8.215\n",
      "wandb:        model/parameters 3014748\n",
      "wandb: model/speed_PyTorch(ms) 0.773\n",
      "wandb:          train/box_loss 1.30353\n",
      "wandb:          train/cls_loss 3.56896\n",
      "wandb:          train/dfl_loss 1.29639\n",
      "wandb:            val/box_loss 1.21714\n",
      "wandb:            val/cls_loss 2.09972\n",
      "wandb:            val/dfl_loss 1.18275\n",
      "wandb: \n",
      "wandb: ðŸš€ View run yolov8n_voc_finetune_demo at: https://wandb.ai/muwayne-vibe-dailygpt/runs-detect/runs/6la5c9wg\n",
      "wandb: â­ï¸ View project at: https://wandb.ai/muwayne-vibe-dailygpt/runs-detect\n",
      "wandb: Synced 5 W&B file(s), 29 media file(s), 10 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250329_140400-6la5c9wg/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning demo completed. Results saved to runs/detect/yolov8n_voc_finetune_demo\n",
      "Best weights saved at: runs/detect/yolov8n_voc_finetune_demo/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if YAML exists and VOC_ROOT is valid\n",
    "if voc_yaml_path.is_file() and VOC_ROOT and Path(VOC_ROOT).is_dir():\n",
    "    try:\n",
    "        # Create a fresh model instance for fine-tuning\n",
    "        finetune_model = YOLO(\"yolov8n.pt\")\n",
    "        print(\"Loaded yolo8n.pt for fine-tuning.\")\n",
    "\n",
    "        # Define training parameters for the demo\n",
    "        # !!! Adjust these for real training !!!\n",
    "        train_results = finetune_model.train(\n",
    "            data=str(voc_yaml_path),  # Path to the dataset YAML\n",
    "            epochs=1,  # DEMO ONLY: Use 50-100+ for real training\n",
    "            imgsz=320,  # DEMO ONLY: Use 640 for real training\n",
    "            batch=8,  # DEMO ONLY: Increase based on GPU memory (e.g., 16, 32)\n",
    "            name=\"yolov8n_voc_finetune_demo\",  # Experiment name (results saved to runs/detect/...)\n",
    "            project=\"runs/detect\",  # Project directory to save runs\n",
    "            exist_ok=True,  # Overwrite existing experiment folder if needed\n",
    "            verbose=True,  # Show detailed training progress\n",
    "            device='0,1'                # Specify GPU device (e.g., 0) or 'cpu'\n",
    "        )\n",
    "        print(\"Fine-tuning demo completed. Results saved to runs/detect/yolov8n_voc_finetune_demo\")\n",
    "\n",
    "        # The best model weights are usually saved as 'best.pt' in the experiment directory\n",
    "        best_weights_path = Path(\"runs/detect/yolov8n_voc_finetune_demo/weights/best.pt\")\n",
    "        if best_weights_path.is_file():\n",
    "            print(f\"Best weights saved at: {best_weights_path}\")\n",
    "        else:\n",
    "            print(\"Could not find best.pt. Check training output.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during fine-tuning: {e}\")\n",
    "else:\n",
    "    print(\"Skipping fine-tuning due to missing YAML file or invalid VOC_ROOT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69b9d0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Evaluating the Fine-tuned Model\n",
    "\n",
    "After fine-tuning (even for 1 epoch), we can evaluate its performance on the validation set defined in `voc.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the fine-tuned weights (use 'best.pt' if it exists)\n",
    "fine_tuned_weights = None\n",
    "if \"best_weights_path\" in locals() and best_weights_path.is_file():\n",
    "    fine_tuned_weights = str(best_weights_path)\n",
    "elif Path(\"runs/detect/yolov8n_voc_finetune_demo/weights/last.pt\").is_file():\n",
    "    # Fallback to last epoch weights if best.pt not found\n",
    "    fine_tuned_weights = \"runs/detect/yolov8n_voc_finetune_demo/weights/last.pt\"\n",
    "    print(\"Using last.pt for evaluation as best.pt was not found.\")\n",
    "\n",
    "if fine_tuned_weights and voc_yaml_path.is_file():\n",
    "    try:\n",
    "        # Load the fine-tuned model\n",
    "        eval_model = YOLO(fine_tuned_weights)\n",
    "        print(f\"Loaded fine-tuned model: {fine_tuned_weights}\")\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        # The validation set is specified in voc.yaml\n",
    "        val_results = eval_model.val(\n",
    "            data=str(voc_yaml_path),\n",
    "            split=\"val\",  # Explicitly specify the validation split\n",
    "            # imgsz=320,             # Can specify image size for validation too\n",
    "            # batch=8                # Can specify batch size for validation\n",
    "            name=\"yolov8n_voc_eval_demo\",  # Separate name for evaluation results\n",
    "        )\n",
    "        # The results object (val_results.box.map50, etc.) holds the metrics\n",
    "        print(\"Validation completed.\")\n",
    "        # print(val_results.box) # Show detailed box metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation due to missing fine-tuned weights or YAML file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8faba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on a random image\n",
    "if \"eval_model\" in locals():\n",
    "    test_image_path_ft = load_random_voc_image(VOC_ROOT)\n",
    "    if test_image_path_ft:\n",
    "        try:\n",
    "            results_ft = eval_model(test_image_path_ft)\n",
    "\n",
    "            # Display the results\n",
    "            res_plotted_ft = results_ft[0].plot()\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(cv2.cvtColor(res_plotted_ft, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Fine-tuned YOLOv8 Results (1 Epoch Demo)\")\n",
    "            plt.show()\n",
    "\n",
    "            # Optionally, print class names (should now be VOC classes)\n",
    "            # print(\"Fine-tuned model classes:\", eval_model.names)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference with fine-tuned model: {e}\")\n",
    "    else:\n",
    "        print(\"Could not load random image for fine-tuned test.\")\n",
    "else:\n",
    "    print(\"Skipping inference with fine-tuned model as it wasn't loaded/trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc35f9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Training YOLOv8 from Scratch (Explanation)\n",
    "\n",
    "Training a YOLOv8 model from scratch involves initializing a model with random weights based on an architecture definition (e.g., `yolo8n.yaml`) instead of loading pre-trained weights (`yolo8n.pt`).\n",
    "\n",
    "The process is similar to fine-tuning, but you specify the model architecture YAML instead of a `.pt` file.\n",
    "\n",
    "```python\n",
    "# Example code structure (DO NOT RUN unless you intend to train from scratch)\n",
    "\n",
    "# # Create a new model from the yolo8n architecture YAML\n",
    "# scratch_model = YOLO('yolo8n.yaml')\n",
    "#\n",
    "# # Train from scratch on the VOC dataset\n",
    "# results_scratch = scratch_model.train(\n",
    "#     data=str(voc_yaml_path),      # Path to your dataset config\n",
    "#     epochs=100,                   # Need significantly more epochs (e.g., 100-300)\n",
    "#     imgsz=640,                    # Standard image size\n",
    "#     batch=16,                     # Adjust based on GPU memory\n",
    "#     name='yolov8n_voc_scratch',   # Experiment name\n",
    "#     project='runs/detect',        # Project directory\n",
    "#     exist_ok=True,\n",
    "#     verbose=True\n",
    "# )\n",
    "```\n",
    "\n",
    "**Key Differences & Considerations for Training from Scratch:**\n",
    "\n",
    "1.  **Initialization:** Starts with `YOLO('yolo8n.yaml')` instead of `YOLO('yolo8n.pt')`. The `.yaml` file defines the network layers and structure. Ultralytics provides standard architecture YAMLs.\n",
    "2.  **Training Time:** Requires significantly more training epochs (e.g., 100-300+) and overall time compared to fine-tuning.\n",
    "3.  **Data Requirement:** Generally needs more data to converge well compared to fine-tuning.\n",
    "4.  **Performance:** May not achieve the same peak performance as fine-tuning a model pre-trained on a large, diverse dataset like COCO, especially if your target dataset is small.\n",
    "5.  **Use Cases:** Useful when your target domain or required architecture differs significantly from the pre-trained models, or when you want to build a model without relying on external pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582bbd5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered the basics of using Ultralytics YOLOv8:\n",
    "\n",
    "1. Running inference with a pre-trained model (`yolo8n.pt`).\n",
    "2. Fine-tuning a pre-trained model on the PASCAL VOC dataset using a configuration YAML.\n",
    "3. Understanding the process for training a model from scratch.\n",
    "\n",
    "Remember to adjust training parameters like `epochs`, `imgsz`, and `batch` based on your specific needs and hardware capabilities. For more advanced features, consult the [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/).\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "vbl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
